{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57KWVOyDzNfo",
        "outputId": "d7f0d9ba-49ab-4a3f-e508-f84bbeb5b812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=48ba56a9d5bceed54320c05a8e8f67a6afbe1060b9cee1b33119f3dc9d091f9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a KMeans Clustering algorithm on a Spark cluster."
      ],
      "metadata": {
        "id": "qu5uBP1v8wNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from pyspark.ml.linalg import Vectors"
      ],
      "metadata": {
        "id": "Pk8_sz4518UQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data.\n",
        "spark = SparkSession.builder.appName('clustering').getOrCreate()\n",
        "dataset =spark.read.format(\"libsvm\").load(\"/content/sample_kmeans_data.txt\")\n",
        "\n",
        "dataset.show(10,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvAi3nssEjKT",
        "outputId": "33c1fa71-5636-41d3-98de-c32048b5fcc5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------------+\n",
            "|label|features                 |\n",
            "+-----+-------------------------+\n",
            "|0.0  |(3,[],[])                |\n",
            "|1.0  |(3,[0,1,2],[0.1,0.1,0.1])|\n",
            "|2.0  |(3,[0,1,2],[0.2,0.2,0.2])|\n",
            "|3.0  |(3,[0,1,2],[9.0,9.0,9.0])|\n",
            "|4.0  |(3,[0,1,2],[9.1,9.1,9.1])|\n",
            "|5.0  |(3,[0,1,2],[9.2,9.2,9.2])|\n",
            "+-----+-------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trains a k-means model.\n",
        "kmeans = KMeans().setK(2).setSeed(1)\n",
        "model = kmeans.fit(dataset)"
      ],
      "metadata": {
        "id": "UimJjc7eFFD4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = model.transform(dataset)"
      ],
      "metadata": {
        "id": "pNhDdqvKFFdd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate clustering by computing Silhouette score\n",
        "\n",
        "evaluator = ClusteringEvaluator()\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
        "\n",
        "\n",
        "# Silhouette score indicates perfect clusters since it is almost 1,\n",
        "# This happened due to using the same dataset while making the predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-H5OnScFFgz",
        "outputId": "6732f00d-b12e-4e77-cee6-6f57cb14351a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette with squared euclidean distance = 0.9997530305375207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Showing the clusters' centroids\n",
        "\n",
        "centers = model.clusterCenters()\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "  print(center)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMU3ZhyD-sTq",
        "outputId": "64a68898-13e3-474f-d58e-5f53bea4819a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Centers: \n",
            "[9.1 9.1 9.1]\n",
            "[0.1 0.1 0.1]\n"
          ]
        }
      ]
    }
  ]
}