{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57KWVOyDzNfo",
        "outputId": "3ab91bd4-5291-4c51-8d10-941a2036d565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=23c1df278a85b3cb8812e36ec0919162355e31769636163e9d44441f7a3adf4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application 1 - Logisitc Regression using text data"
      ],
      "metadata": {
        "id": "qu5uBP1v8wNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "training = spark.createDataFrame([\n",
        "(0, \"This is a testing for spark\", 1.0),\n",
        "(1, \"kudu ozone\", 0.0),\n",
        "(2, \"spark is in-memory engine\", 1.0),\n",
        "(3, \"hive is data warehouse\", 0.0),\n",
        "(4, \"hadoop is mapreduce for batch\", 0.0)\n",
        "], [\"id\", \"text\", \"label\"])"
      ],
      "metadata": {
        "id": "Pk8_sz4518UQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configure the Spark machine learning pipeline**\n",
        "\n",
        " This Spark machine learning pipeline consists of three stages.\n",
        "*   The tokenizer Transformer stage takes in raw text and converts them to words DataFrame.\n",
        "*   The hashingTF Transformer stage takes those words and creates a feature\n",
        "vector DataFrame.\n",
        "*   Finally, the logistic regression Estimator takes in the feature vectors and fits them to create a new model – which is a Transformer.\n",
        "\n"
      ],
      "metadata": {
        "id": "KUwFAeWx2l6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
      ],
      "metadata": {
        "id": "jSa9DzrS3GKS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the pipeline to the training document and verify the training data.\n",
        "\n",
        "model = pipeline.fit(training)\n",
        "training.show(5,False)\n",
        "\n",
        "\n",
        "# We can see that sentences containing the word 'spark' will have a label of 1 , o.w , it will be zero"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVlZ44P53XAp",
        "outputId": "58a72536-58e8-4011-9660-29cb2291651e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------------------+-----+\n",
            "|id |text                         |label|\n",
            "+---+-----------------------------+-----+\n",
            "|0  |This is a testing for spark  |1.0  |\n",
            "|1  |kudu ozone                   |0.0  |\n",
            "|2  |spark is in-memory engine    |1.0  |\n",
            "|3  |hive is data warehouse       |0.0  |\n",
            "|4  |hadoop is mapreduce for batch|0.0  |\n",
            "+---+-----------------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare data set to run against trained model**\n",
        "\n",
        "\n",
        "*   The model accepts a id and text as input.\n",
        "*   The model predicts whether a sentence with ID and text contains spark (1.0) or not\n",
        "(0.0).\n",
        "\n"
      ],
      "metadata": {
        "id": "QLH1qKdY4gtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import 'Row' class to create row objects in our spark data frame\n",
        "\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Starting spark context from the spark session\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "pD4pRojk4754"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Document = Row(\"id\", \"text\")\n",
        "test = sc.parallelize([(5, \"K O 1\"),\n",
        "(6, \"spark hadoop spark impala\"),\n",
        "(7, \"apache spark open-source\"),\n",
        "(8, \"spark is a platform\"),\n",
        "(9, \"Hadoop is for Big Data\")]).map(lambda x: Document(*x)).toDF()"
      ],
      "metadata": {
        "id": "6swzmXta4wZF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make prediction on test data**\n",
        "\n",
        "\n",
        "*   model is the new Transformer that resulted from executing the pipeline.\n",
        "*   I will now use the transform method of the newly created predictor model against\n",
        "the test data.\n",
        "\n"
      ],
      "metadata": {
        "id": "cc1C_2Wt6Co-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test documents and print columns of interest\n",
        "prediction = model.transform(test)\n",
        "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
        "for row in selected.collect():\n",
        "    rid, text, prob, prediction = row\n",
        "    print(\n",
        "    \"(%d, %s) --> prob=%s, prediction=%f\" % (\n",
        "    rid, text, str(prob), prediction\n",
        "    )\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L75Fi_P6G8n",
        "outputId": "c343a803-51ae-4c66-a01b-6e36175d6733"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, K O 1) --> prob=[0.9719231922061782,0.028076807793821823], prediction=0.000000\n",
            "(6, spark hadoop spark impala) --> prob=[0.30207973983131137,0.6979202601686887], prediction=1.000000\n",
            "(7, apache spark open-source) --> prob=[0.6450881900811496,0.3549118099188504], prediction=0.000000\n",
            "(8, spark is a platform) --> prob=[0.064777768484776,0.935222231515224], prediction=1.000000\n",
            "(9, Hadoop is for Big Data) --> prob=[0.9922736248651602,0.007726375134839758], prediction=0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Using the 0.5 threshold for classification, we can see that the model is reasonably well at predicting whether the sentence contains 'spark' or not\n",
        "\n"
      ],
      "metadata": {
        "id": "xBVLz_vy6xVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application 2 - Logisitc Regression using vectors"
      ],
      "metadata": {
        "id": "h5EeZEof9Jn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors"
      ],
      "metadata": {
        "id": "1rsL8FNv7Nsw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training data with label and features\n",
        "training = spark.createDataFrame([\n",
        "(1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
        "(0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
        "(0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
        "(1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])"
      ],
      "metadata": {
        "id": "0EczCsvC9Ycf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an instance of LogisticRegreesion (an estimator for now)\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
        "\n",
        "# Creating model with the parameters stored in lr.\n",
        "Model1=lr.fit(training)"
      ],
      "metadata": {
        "id": "1WvsHae69Zad"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the training document to checkpoint the application progress\n",
        "\n",
        "training.show(10,False)\n",
        "\n",
        "# Now model1 is a transformer produced by an Estimator."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXfB0FJh9Zga",
        "outputId": "a64919cc-8d41-4677-df75-4ff6ac6b1d31"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------+\n",
            "|label|features      |\n",
            "+-----+--------------+\n",
            "|1.0  |[0.0,1.1,0.1] |\n",
            "|0.0  |[2.0,1.0,-1.0]|\n",
            "|0.0  |[2.0,1.3,1.0] |\n",
            "|1.0  |[0.0,1.2,-0.5]|\n",
            "+-----+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paramMap = {lr.maxIter: 20}\n",
        "paramMap[lr.maxIter] = 30 # Specify 1 Param, overwriting the original maxIter.\n",
        "# Specify multiple Params.\n",
        "paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55})\n",
        "paramMap2 = {lr.probabilityCol: \"myProbability\"} # type: ignore\n",
        "paramMapCombined = paramMap.copy()\n",
        "paramMapCombined.update(paramMap2) # type: ignore\n",
        "\n",
        "# paramMapCombined overrides all parameters set earlier via lr.set* methods.\n",
        "model2 = lr.fit(training, paramMapCombined)"
      ],
      "metadata": {
        "id": "gk6jj8y2-hRC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data set to run against trained model. And make a prediction on test data.\n",
        "\n",
        "test = spark.createDataFrame([\n",
        "(1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
        "(0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
        "(1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n",
        "\n",
        "\n",
        "# Make predictions on test data using the Transformer.transform() method.\n",
        "# LogisticRegression.transform will only use the 'features' column.\n",
        "# Note that model2.transform() outputs a \"myProbability\" column instead of the usual\n",
        "# 'probability' column since we renamed the lr.probabilityCol parameter previously.\n",
        "prediction = model2.transform(test)\n",
        "result = prediction.select(\"features\", \"label\", \"myProbability\", \"prediction\") \\\n",
        ".collect()"
      ],
      "metadata": {
        "id": "uuzxzDvx-r9p"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Observing the outcome after prediction\n",
        "\n",
        "for row in result:\n",
        "    print(\"features=%s, label=%s -> prob=%s, prediction=%s\"\n",
        "  % (row.features, row.label, row.myProbability, row.prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymMOMRm6-sQO",
        "outputId": "68dcffb0-6348-48a4-f8b1-a4495016d598"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features=[-1.0,1.5,1.3], label=1.0 -> prob=[0.05707304993572537,0.9429269500642746], prediction=1.0\n",
            "features=[3.0,2.0,-0.1], label=0.0 -> prob=[0.9238521956443227,0.07614780435567725], prediction=0.0\n",
            "features=[0.0,2.2,-1.5], label=1.0 -> prob=[0.10972780286187774,0.8902721971381222], prediction=1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XMU3ZhyD-sTq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}